{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ZWCNi_Djwr"
      },
      "source": [
        "<table style=\"border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9; width: 95%; margin-bottom: 20px;\" align=\"left\">\n",
        "  <tr>\n",
        "    <td style=\"padding: 15px; text-align: center;\">\n",
        "      <a target=\"_blank\" href=\"https://github.com/AbdallaNassar\">\n",
        "        <img src=\"https://raw.githubusercontent.com/detain/svg-logos/b02ee1ac30c7ff4757278337c95588b01ed0954b/svg/g/github-icon-2.svg\" width=\"50px\" alt=\"GitHub\" />\n",
        "      </a>\n",
        "    </td>\n",
        "    <td style=\"padding: 15px; text-align: center;\">\n",
        "      <a target=\"_blank\" href=\"https://abdallanassar.me/\">\n",
        "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAAXNSR0IArs4c6QAAAtpJREFUeF7tms9rE0EUx79vim1SFPXS3Y1GKnhT8D9QPPr7YEFBPHjyJG3FJB6Dp25bK+rJm6B4KEJ7kIp/gd705MWDP2g3myp4CdY0Ok82GEiK7STphGZ2Z8+zw34/833z3rwdQsIfSrh+WADWAQknYEMg4Qawm6ANARsCCSdgQyDhBrBZQBkCYc45x0SP604hTHp+OB8n16gBFJxlsDgQiWaABXHe8cPZuEDoCEBDtCQ88NKlW1SENB1EGwAyFyHlPIgGm8WSxIufu4euHS5+/mUyBCWASFwp754CiwUi3tsslpnessT5zL3gu6kQ2gJQh1BwjxLjFUDZFrGEDxJ0OuMHX02E0DaASNy3fDZTk7+XhMDxFicApQFBZ0emgnemQegIQCTux8TovupgdRHAyRYIEhUxgDHHL702CULHACJxH28eGdqTrjwB6EqLWOZ1CHHd9YPnpkDoCsC/moBWc+4UE+U3hAMDdNebDoomQOgaQENcmPPGGZgjgmjdHPmRkw4n+r1W2DaASHQ5511i8DMQpVohyIXa+q6r2fvLa/3qBi0ATK4VtAEwtVbQCsDEWkE7ANNqhZ4AiCB8Ko6mhivVpywwtrFWIBKXnekgKqZ2/OkZgHqtUIQorXlzgjG+ITusuH754I6rr/d4evgkGkCiQ+DLnUP7U7K2yMCJfj8waQ+BLY/MLM6MzKy872HUdTy1VgDhbecYBC2Z1DTRBmDTthn4Df8RF/q1baYFQKIPQ5seh4GHznBpMrbH4ajrsZrP+AzOJa4hkuiWmEk5vp2c2NEmaFqO1wrAxByvDYCpOV4LgLCg9+do474BgTLtfGC3YxgcEPMNd6b8cqs5lHtA2HQ/oDHRdn6P/2++bkUq3yOp7Dt0BEDHBQnzAGi+ImNcCChtZvgAZQgYrk/5+RaAElHMB1gHxHyBlfKsA5SIYj7AOiDmC6yUZx2gRBTzAdYBMV9gpby/3TN6UC20cF8AAAAASUVORK5CYII=\" width=\"50px\" alt=\"Website\" />\n",
        "      </a>\n",
        "    </td>\n",
        "    <td style=\"padding: 15px; text-align: center;\">\n",
        "      <a target=\"_blank\" href=\"https://www.kaggle.com/abdallanassar25\">\n",
        "        <img src=\"https://www.vectorlogo.zone/logos/kaggle/kaggle-ar21.svg\" width=\"150px\" alt=\"Kaggle\" />\n",
        "      </a>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5FEOdsnQw69"
      },
      "source": [
        "## If you need use GUI only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zztMvIckQ9G-"
      },
      "source": [
        "<h1>load first 3 Cells and GUI and Logic Cells</h1>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0e80f8Rl4vi"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9qc7MBpmhsD"
      },
      "source": [
        "## Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JynqwzOxGDmj",
        "outputId": "770f900d-c7dd-402f-de91-7ff8fa0c78be"
      },
      "outputs": [],
      "source": [
        "# Update packages\n",
        "! pip install -U langchain_community tiktoken faiss-cpu langchain langchainhub sentence_transformers BeautifulSoup4 -q\n",
        "\n",
        "# Install pypdf\n",
        "! pip install pypdf -q\n",
        "\n",
        "# Install langchain-huggingface\n",
        "! pip install langchain-huggingface -q\n",
        "\n",
        "# Upgrade langchain and langchain-google\n",
        "! pip install --upgrade langchain langchain-google-q\n",
        "\n",
        "# Install langchain-google-genai version\n",
        "!pip install langchain_google_genai -q\n",
        "\n",
        "# # Install google-generativeai\n",
        "! pip install google-generativeai-q\n",
        "! pip install gradio -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MCJ4_eB3Tj7Z"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import gradio as gr\n",
        "import os,re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8J81XFmGRMu"
      },
      "source": [
        "## Generative AI setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N7xTGEf1GMJp"
      },
      "outputs": [],
      "source": [
        "# Initialize ChatGoogleGenerativeAI for generative AI chat\n",
        "def get_llm(model_name):\n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=model_name,\n",
        "        temperature=0.1,\n",
        "        max_output_tokens=500,\n",
        "        google_api_key= \"your_api_key_here\" \n",
        "    )\n",
        "\n",
        "template = \"\"\"You are an expert AI instructor tasked with evaluating student answers. Use the following reference information to assess the student's answer:\n",
        "\n",
        "Reference Information: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Student's Answer: {student_answer}\n",
        "\n",
        "Evaluate the student's answer based on the reference information. Provide:\n",
        "1. A numerical grade out of 10\n",
        "2. A letter grade based on the following scale:\n",
        "   9-10: A+\n",
        "   8-8.9: A\n",
        "   7-7.9: B+\n",
        "   6-6.9: B\n",
        "   5-5.9: C+\n",
        "   4-4.9: C\n",
        "   3-3.9: D+\n",
        "   2-2.9: D\n",
        "   0-2.9: F\n",
        "3. Detailed feedback explaining the rating\n",
        "4. Suggestions for improvement\n",
        "\n",
        "Your response should be in the following format:\n",
        "Numerical Grade: [Your grade out of 10]\n",
        "Letter Grade: [Corresponding letter grade]\n",
        "Feedback: [Your detailed feedback]\n",
        "Suggestions: [Your suggestions for improvement]\n",
        "\n",
        "Please provide your response in {language}. When responding in Arabic i need response like this\n",
        "\n",
        "Numerical Grade= Numerical Grade: 10\n",
        "Letter Grade= Letter Grade: A+\n",
        "Feedback= Feedback: الاجابه بالعربي\n",
        "Suggestions= Suggestions: الاجابه بالعربي\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLov9cAbmlDE"
      },
      "source": [
        "## Loading PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjMWEbGfGL_P"
      },
      "outputs": [],
      "source": [
        "# Initialize the PyPDFLoader with your PDF file path\n",
        "loader = PyPDFLoader(\"/content/Understanding Artificial Intelligence.pdf\")\n",
        "\n",
        "# Load the data from the PDF\n",
        "data = loader.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwB5IyPJmuKC"
      },
      "source": [
        "## Splitting documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NaNE4kdHNFM"
      },
      "outputs": [],
      "source": [
        "# Initialize the RecursiveCharacterTextSplitter with desired chunk parameters\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "\n",
        "# Split the document into chunks\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddjZYOmAmwvh"
      },
      "source": [
        "## Embeddings and vector store creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inSwiy8RIaMx",
        "outputId": "664cadaf-ae1b-4949-d244-01f0f7b98e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize HuggingFaceEmbeddings with the model name\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H33uX_uIysB"
      },
      "outputs": [],
      "source": [
        "# Create FAISS vector store from documents and embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
        "\n",
        "# Convert vector store to a retriever\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2UtDPqYm34w"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLBHNkTum8uu"
      },
      "source": [
        "## Similarity search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94wCv5aRI6Gh",
        "outputId": "26364583-0455-4992-bdcc-aa4cb46c6cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents retrieved: 4\n"
          ]
        }
      ],
      "source": [
        "# Perform similarity search with the question\n",
        "question = \"What is Artificial Intelligence?\"\n",
        "docs = vectorstore.similarity_search(question)\n",
        "\n",
        "# Get the number of documents retrieved\n",
        "num_docs = len(docs)\n",
        "\n",
        "print(f\"Number of documents retrieved: {num_docs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-HrsezaJjwI",
        "outputId": "244d7701-958a-45c6-dbee-cef4fedb5cef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/Understanding Artificial Intelligence.pdf', 'page': 1}, page_content='Introduction to Artificial Intelligence \\nDefinition of Artificial Intelligence (AI): \\nAI refers to the simulation of human intelligence in machines that are programmed \\nto think and learn like humans. It encompasses a variety of technologies that \\nenable machines to perform tasks that typically require human intelligence.')"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMiOxfktnGQC"
      },
      "source": [
        "## Chaining processes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzuViAJaWZ79"
      },
      "outputs": [],
      "source": [
        "# Define a function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Define a function to combine retriever results with input\n",
        "def combine_inputs(input_dict):\n",
        "    question = input_dict[\"question\"]\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    formatted_context = format_docs(retrieved_docs)\n",
        "    return {\n",
        "        \"context\": formatted_context,\n",
        "        \"question\": question,\n",
        "        \"student_answer\": input_dict[\"student_answer\"]\n",
        "    }\n",
        "\n",
        "# Chain\n",
        "chain = (\n",
        "    RunnablePassthrough()\n",
        "    | combine_inputs\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXFvCISZnNZU"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8vSZ5igkuKu"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "question = \"Compare and contrast machine learning with traditional programming approaches. In what scenarios is machine learning more advantageous, and what limitations does it have compared to conventional programming methods?\"\n",
        "student_answer = \"Machine learning (ML) represents a shift from traditional programming methods. In traditional programming, developers write explicit instructions for the computer to follow. In contrast, ML algorithms learn from data and can adapt over time. This makes ML particularly effective for tasks like speech recognition and image classification. However, ML requires large datasets and can be sensitive to the quality of the data it learns from, which can lead to issues like overfitting.\"\n",
        "\n",
        "result = chain.invoke({\"question\": question, \"student_answer\": student_answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "C39gnPCt_6kD",
        "outputId": "ce71a960-7043-4458-aa81-67dcd332416b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Grade: 7/10\\n\\nFeedback: The student demonstrates a good understanding of the fundamental differences between machine learning and traditional programming. They accurately highlight the key aspects of both approaches, including the reliance on explicit instructions in traditional programming versus data-driven learning in ML. The student also correctly identifies the strengths of ML in handling complex tasks like speech recognition and image classification. \\n\\nThe answer further acknowledges a significant limitation of ML, namely the need for large datasets and the potential for overfitting. However, the explanation could be more detailed. \\n\\nSuggestions:\\n\\n* **Expand on the limitations:**  The student mentions overfitting but could elaborate on other limitations like the \"black box\" nature of ML models (difficulty in understanding the decision-making process), the need for specialized expertise in ML, and the potential for bias in the training data.\\n* **Provide specific examples:**  Instead of just mentioning speech recognition and image classification, the student could provide concrete examples of how ML excels in these areas. For instance, they could mention how ML powers voice assistants like Siri or Alexa or how it\\'s used in self-driving cars for object detection.\\n* **Compare the flexibility:** The answer briefly touches on ML\\'s adaptability but could further contrast it with the rigidity of traditional programming. For example, the student could explain how ML models can be updated with new data without requiring code changes, while traditional programs often need significant modifications for even minor adjustments.\\n* **Discuss scenarios:** The question asks for scenarios where ML is more advantageous. The student could expand on this by providing specific examples of problems that are better suited for ML, such as fraud detection, medical diagnosis, or personalized recommendations. \\n\\nBy incorporating these suggestions, the student can provide a more comprehensive and insightful comparison of machine learning and traditional programming. \\n'"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbY_OkVSnpLi"
      },
      "source": [
        "## Extracting Grade, Feedback, and Suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWkalvOJnmGB",
        "outputId": "b24fc095-efa4-43f3-a3a3-dfeaf7b4e964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grade: Grade: 7/10\n",
            "Feedback: The student demonstrates a good understanding of the fundamental differences between machine learning and traditional programming. They accurately highlight the key aspects of both approaches, including the reliance on explicit instructions in traditional programming versus data-driven learning in ML. The student also correctly identifies the strengths of ML in handling complex tasks like speech recognition and image classification. \n",
            "\n",
            "The answer further acknowledges a significant limitation of ML, namely the need for large datasets and the potential for overfitting. However, the explanation could be more detailed.\n",
            "Suggestions: * **Expand on the limitations:**  The student mentions overfitting but could elaborate on other limitations like the \"black box\" nature of ML models (difficulty in understanding the decision-making process), the need for specialized expertise in ML, and the potential for bias in the training data.\n",
            "* **Provide specific examples:**  Instead of just mentioning speech recognition and image classification, the student could provide concrete examples of how ML excels in these areas. For instance, they could mention how ML powers voice assistants like Siri or Alexa or how it's used in self-driving cars for object detection.\n",
            "* **Compare the flexibility:** The answer briefly touches on ML's adaptability but could further contrast it with the rigidity of traditional programming. For example, the student could explain how ML models can be updated with new data without requiring code changes, while traditional programs often need significant modifications for even minor adjustments.\n",
            "* **Discuss scenarios:** The question asks for scenarios where ML is more advantageous. The student could expand on this by providing specific examples of problems that are better suited for ML, such as fraud detection, medical diagnosis, or personalized recommendations. \n",
            "\n",
            "By incorporating these suggestions, the student can provide a more comprehensive and insightful comparison of machine learning and traditional programming.\n"
          ]
        }
      ],
      "source": [
        "# Extract grade, feedback, and suggestions\n",
        "grade = result.split('Feedback:')[0].strip()\n",
        "feedback = result.split('Feedback:')[1].split('Suggestions:')[0].strip()\n",
        "suggestions = result.split('Suggestions:')[1].strip()\n",
        "\n",
        "# Print extracted values\n",
        "print(f\"Grade: {grade}\")\n",
        "print(f\"Feedback: {feedback}\")\n",
        "print(f\"Suggestions: {suggestions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl7XMWv3-Gzn"
      },
      "source": [
        "# GUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSAR-D5c-PYF"
      },
      "source": [
        "## Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lxrIpYjP2Ypj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a function to process the input and generate the output\n",
        "def predict(pdf_file, question, student_answer, model_name, grade_type, language):\n",
        "    # Load the PDF\n",
        "    loader = PyPDFLoader(pdf_file.name)\n",
        "    data = loader.load()\n",
        "\n",
        "    # Split the document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "    all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "    # Create FAISS vector store from documents and embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # Define a function to format retrieved documents\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Define a function to combine retriever results with input\n",
        "    def combine_inputs(input_dict):\n",
        "        question = input_dict[\"question\"]\n",
        "        retrieved_docs = retriever.get_relevant_documents(question)\n",
        "        formatted_context = format_docs(retrieved_docs)\n",
        "        return {\n",
        "            \"context\": formatted_context,\n",
        "            \"question\": question,\n",
        "            \"student_answer\": input_dict[\"student_answer\"],\n",
        "            \"language\": input_dict[\"language\"]\n",
        "        }\n",
        "\n",
        "    # Get the LLM based on the selected model\n",
        "    llm = get_llm(model_name)\n",
        "\n",
        "    # Chain\n",
        "    chain = (\n",
        "        RunnablePassthrough()\n",
        "        | combine_inputs\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Invoke the chain\n",
        "    output = chain.invoke({\"question\": question, \"student_answer\": student_answer, \"language\": language})\n",
        "\n",
        "    patterns = {\n",
        "        'numerical_grade': r'Numerical Grade:\\s*([\\d.]+)',\n",
        "        'letter_grade': r'Letter Grade:\\s*([A-F][+]?)',\n",
        "        'feedback': r'Feedback:\\s*(.+?)(?=\\n\\S|$)',\n",
        "        'suggestions': r'Suggestions:\\s*(.+?)(?=\\n\\S|$)'\n",
        "    }\n",
        "\n",
        "    # Extract information using regex\n",
        "    extracted_info = {}\n",
        "    for key, pattern in patterns.items():\n",
        "        match = re.search(pattern, output, re.DOTALL)\n",
        "        extracted_info[key] = match.group(1).strip() if match else 'Information not found'\n",
        "\n",
        "    # Select the grade based on user choice\n",
        "    grade = extracted_info['numerical_grade'] if grade_type == \"Numerical Grade\" else extracted_info['letter_grade']\n",
        "\n",
        "    # Prepare the output\n",
        "    feedback = extracted_info['feedback']\n",
        "    suggestions = extracted_info['suggestions']\n",
        "\n",
        "    # If no structured information was found, return the raw output\n",
        "    if all(value == 'Information not found' for value in extracted_info.values()):\n",
        "        grade = \"Unable to parse grade\"\n",
        "        feedback = \"Unable to parse feedback. Raw output:\\n\\n\" + output\n",
        "        suggestions = \"Unable to parse suggestions\"\n",
        "\n",
        "    return grade, feedback, suggestions, model_name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66PuN97y-RtT"
      },
      "source": [
        "## Gui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "1IqUs83w-YOO",
        "outputId": "9ff3bc64-936b-4c1d-908d-cb73c3e165ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://47c771a00b7ff0bb50.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://47c771a00b7ff0bb50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF\"),\n",
        "        gr.Textbox(label=\"Question\"),\n",
        "        gr.Textbox(label=\"Student Answer\"),\n",
        "        gr.Dropdown(\n",
        "            choices=[\"gemini-1.5-flash\", \"gemini-1.0-pro\",\"gemini-1.5-pro\"],\n",
        "            label=\"Select Gemini Model , If you use Arabic make model is gemini-1.5-flash\",\n",
        "            value=\"gemini-1.5-flash\"\n",
        "        ),\n",
        "        gr.Radio(\n",
        "            choices=[\"Numerical Grade\", \"Letter Grade\"],\n",
        "            label=\"Grade Type\",\n",
        "            value=\"Numerical Grade\"\n",
        "        ),\n",
        "        gr.Radio(\n",
        "            choices=[\"English\", \"Arabic\"],\n",
        "            label=\"Language\",\n",
        "            value=\"English\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Grade\"),\n",
        "        gr.Textbox(label=\"Feedback\"),\n",
        "        gr.Textbox(label=\"Suggestions\")\n",
        "    ],\n",
        "    title=\"AI-Powered Student Assessment\",\n",
        "    description=\"Evaluate student answers using AI models and provide grades, feedback, and suggestions.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RleR0sW2JgF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
